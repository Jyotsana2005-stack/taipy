# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler

# Loading the dataset
df = pd.read_csv("heart_failure_clinical_records_dataset.csv")

# Checking the first few rows of the dataset
print("Dataset Head:\n", df.head())

# Checking the data info (missing values, data types)
print("\nData Info:\n", df.info())

# Statistical summary of the dataset
print("\nStatistical Summary:\n", df.describe())

# Checking for missing values
missing_values = df.isnull().sum()
print("\nMissing Values:\n", missing_values)

# Visualizing the target variable distribution (DEATH_EVENT)
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x='DEATH_EVENT')
plt.title('Distribution of DEATH_EVENT (Heart Failure Outcome)')
plt.show()

# Visualizing correlations between features
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Splitting features (X) and target (y)
X = df.drop(columns=['DEATH_EVENT'])
y = df['DEATH_EVENT']

# Splitting the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Building the Random Forest Classifier model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Training the model
rf_model.fit(X_train_scaled, y_train)

# Predicting on the test set
y_pred = rf_model.predict(X_test_scaled)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy: {accuracy * 100:.2f}%")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", cm)

# Classification report
report = classification_report(y_test, y_pred)
print("\nClassification Report:\n", report)

# Feature importance visualization
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)
feature_importances.nlargest(10).plot(kind='barh', figsize=(10, 6))
plt.title('Top 10 Most Important Features for Heart Failure Prediction')
plt.show()

# Saving the model using joblib
import joblib
joblib.dump(rf_model, 'heart_failure_predictor_model.pkl')
print("\nModel saved as 'heart_failure_predictor_model.pkl'")

# Loading the saved model
loaded_model = joblib.load('heart_failure_predictor_model.pkl')

# Predicting on new data
new_data = np.array([60, 1, 130, 200, 20, 25, 1, 5, 1, 0, 0, 1, 1, 10]).reshape(1, -1)
new_data_scaled = scaler.transform(new_data)
prediction = loaded_model.predict(new_data_scaled)

print("Prediction (0 = Survived, 1 = Death due to Heart Failure):", prediction[0])
